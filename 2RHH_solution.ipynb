{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving a Lunar Lander with differentiable Genetic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "To install the required libraries run the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-display\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Imports from the standard genepro-multi library are done here. Any adjustments (e.g. different operators) should be made in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from genepro.node_impl import *\n",
    "from genepro.evo import Evolution\n",
    "from genepro.node_impl import Constant\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Setup\n",
    "Here we first setup the Gymnasium environment. Please see https://gymnasium.farama.org/environments/box2d/lunar_lander/ for more information on the environment. \n",
    "\n",
    "Then a memory buffer is made. This is a buffer in which state transitions are stored. When the buffer reaches its maximum capacity old transitions are replaced by new ones.\n",
    "\n",
    "A frame buffer is initialised used to later store animation frames of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "      self.memory += other.memory\n",
    "      return self \n",
    "\n",
    "    def __add__(self, other):\n",
    "      self.memory = self.memory + other.memory \n",
    "      return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Function\n",
    "\n",
    "Here you get to be creative. The default setup evaluates 5 episodes of 300 frames. Think of what action to pick and what fitness function to use. The Multi-tree takes an input of $n \\times d$ where $n$ is a batch of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_function_pt(multitree, generation, num_episodes=5, episode_duration=300, render=False, ignore_done=False):\n",
    "  memory = ReplayMemory(10000)\n",
    "  rewards = []\n",
    "\n",
    "  for i in range(num_episodes):\n",
    "    # get initial state of the environment\n",
    "    observation = env.reset(seed=(generation*1000 + i))\n",
    "    observation = observation[0]\n",
    "    \n",
    "    for _ in range(episode_duration):\n",
    "      if render:\n",
    "        frames.append(env.render())\n",
    "\n",
    "      input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "      \n",
    "      action = torch.argmax(multitree.get_output_pt(input_sample))\n",
    "      observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "      rewards.append(reward)\n",
    "      output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "      memory.push(input_sample, torch.tensor([[action.item()]]), output_sample, torch.tensor([reward]))\n",
    "      if (terminated or truncated) and not ignore_done:\n",
    "        break\n",
    "\n",
    "  fitness = np.sum(rewards)/num_episodes\n",
    "  return fitness, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution Setup & Evolve\n",
    "Here the leaf and internal nodes are defined. Think about the odds of sampling a constant in this default configurations. Also think about any operators that could be useful and add them here. \n",
    "\n",
    "Adjust the population size (multiple of 8 if you want to use the standard tournament selection), max generations and max tree size to taste. Be aware that each of these settings can increase the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 1\n",
      "\tBest - \tFitness: -67.900, Size: 10\n",
      "\tPopulation - \tAverage Fitness: -223.72788705685633\n",
      "\tmean = -223.73, \tmedian = -129.02, \tbest_median = -129.02\n",
      "\n",
      "Generation: 2\n",
      "\tBest - \tFitness: -90.212, Size: 6\n",
      "\tPopulation - \tAverage Fitness: -269.8316423342146\n",
      "\tmean = -269.83, \tmedian = -164.79, \tbest_median = -129.02\n",
      "\n",
      "Generation: 3\n",
      "\tBest - \tFitness: -27.439, Size: 13\n",
      "\tPopulation - \tAverage Fitness: -199.87830169763993\n",
      "\tmean = -199.88, \tmedian = -143.58, \tbest_median = -129.02\n",
      "\n",
      "Generation: 4\n",
      "\tBest - \tFitness: -84.546, Size: 23\n",
      "\tPopulation - \tAverage Fitness: -231.4554730966472\n",
      "\tmean = -231.46, \tmedian = -149.45, \tbest_median = -129.02\n",
      "\n",
      "Generation: 5\n",
      "\tBest - \tFitness: -51.153, Size: 12\n",
      "\tPopulation - \tAverage Fitness: -182.64401479550324\n",
      "\tmean = -182.64, \tmedian = -107.86, \tbest_median = -107.86\n",
      "\n",
      "Generation: 6\n",
      "\tBest - \tFitness: -40.668, Size: 12\n",
      "\tPopulation - \tAverage Fitness: -209.33172631762253\n",
      "\tmean = -209.33, \tmedian = -96.50, \tbest_median = -96.50\n",
      "\n",
      "Generation: 7\n",
      "\tBest - \tFitness: -42.448, Size: 10\n",
      "\tPopulation - \tAverage Fitness: -213.40997692652576\n",
      "\tmean = -213.41, \tmedian = -114.83, \tbest_median = -96.50\n",
      "\n",
      "Generation: 8\n",
      "\tBest - \tFitness: -31.360, Size: 12\n",
      "\tPopulation - \tAverage Fitness: -181.40495949691075\n",
      "\tmean = -181.40, \tmedian = -100.45, \tbest_median = -96.50\n",
      "\n",
      "Generation: 9\n",
      "\tBest - \tFitness: -26.217, Size: 12\n",
      "\tPopulation - \tAverage Fitness: -156.50495651377145\n",
      "\tmean = -156.50, \tmedian = -74.77, \tbest_median = -74.77\n",
      "\n",
      "Generation: 10\n",
      "\tBest - \tFitness: -5.311, Size: 14\n",
      "\tPopulation - \tAverage Fitness: -151.95557897402256\n",
      "\tmean = -151.96, \tmedian = -78.83, \tbest_median = -74.77\n",
      "\n",
      "Generation: 11\n",
      "\tBest - \tFitness: -22.518, Size: 10\n",
      "\tPopulation - \tAverage Fitness: -164.7063915522288\n",
      "\tmean = -164.71, \tmedian = -106.05, \tbest_median = -74.77\n",
      "\n",
      "Generation: 12\n",
      "\tBest - \tFitness: 56.173, Size: 14\n",
      "\tPopulation - \tAverage Fitness: -174.4508023289912\n",
      "\tmean = -174.45, \tmedian = -33.64, \tbest_median = -33.64\n",
      "\n",
      "Generation: 13\n",
      "\tBest - \tFitness: 5.809, Size: 16\n",
      "\tPopulation - \tAverage Fitness: -141.31934894381692\n",
      "\tmean = -141.32, \tmedian = -35.65, \tbest_median = -33.64\n",
      "\n",
      "Generation: 14\n",
      "\tBest - \tFitness: 17.655, Size: 14\n",
      "\tPopulation - \tAverage Fitness: -133.9100698209177\n",
      "\tmean = -133.91, \tmedian = -25.66, \tbest_median = -25.66\n",
      "\n",
      "Generation: 15\n",
      "\tBest - \tFitness: 42.099, Size: 10\n",
      "\tPopulation - \tAverage Fitness: -114.57947344324722\n",
      "\tmean = -114.58, \tmedian = -39.61, \tbest_median = -25.66\n",
      "\n",
      "Generation: 16\n",
      "\tBest - \tFitness: 70.908, Size: 14\n",
      "\tPopulation - \tAverage Fitness: -94.80946397902369\n",
      "\tmean = -94.81, \tmedian = -20.35, \tbest_median = -20.35\n",
      "\n",
      "Generation: 17\n",
      "\tBest - \tFitness: 12.002, Size: 14\n",
      "\tPopulation - \tAverage Fitness: -103.87393350028455\n",
      "\tmean = -103.87, \tmedian = -39.13, \tbest_median = -20.35\n",
      "\n",
      "Generation: 18\n",
      "\tBest - \tFitness: 57.747, Size: 14\n",
      "\tPopulation - \tAverage Fitness: -118.6157902823073\n",
      "\tmean = -118.62, \tmedian = -34.65, \tbest_median = -20.35\n",
      "\n",
      "Generation: 19\n",
      "\tBest - \tFitness: 39.277, Size: 14\n",
      "\tPopulation - \tAverage Fitness: -107.8290356617202\n",
      "\tmean = -107.83, \tmedian = -68.43, \tbest_median = -20.35\n",
      "\n",
      "Generation: 20\n",
      "\tBest - \tFitness: 50.872, Size: 21\n",
      "\tPopulation - \tAverage Fitness: -111.50580242610074\n",
      "\tmean = -111.51, \tmedian = -61.45, \tbest_median = -20.35\n",
      "\n",
      "Generation: 21\n",
      "\tBest - \tFitness: -12.549, Size: 30\n",
      "\tPopulation - \tAverage Fitness: -116.05899818384088\n",
      "\tmean = -116.06, \tmedian = -63.81, \tbest_median = -20.35\n",
      "\n",
      "Generation: 22\n",
      "\tBest - \tFitness: 30.742, Size: 15\n",
      "\tPopulation - \tAverage Fitness: -96.68937123158317\n",
      "\tmean = -96.69, \tmedian = -46.07, \tbest_median = -20.35\n",
      "\n",
      "Generation: 23\n",
      "\tBest - \tFitness: 33.854, Size: 22\n",
      "\tPopulation - \tAverage Fitness: -101.7905901300744\n",
      "\tmean = -101.79, \tmedian = -41.75, \tbest_median = -20.35\n",
      "\n",
      "Generation: 24\n",
      "\tBest - \tFitness: 22.347, Size: 12\n",
      "\tPopulation - \tAverage Fitness: -78.64439764077198\n",
      "\tmean = -78.64, \tmedian = -35.66, \tbest_median = -20.35\n",
      "\n",
      "Generation: 25\n",
      "\tBest - \tFitness: 80.167, Size: 21\n",
      "\tPopulation - \tAverage Fitness: -54.690103765990145\n",
      "\tmean = -54.69, \tmedian = 12.94, \tbest_median = 12.94\n",
      "\n",
      "Generation: 26\n",
      "\tBest - \tFitness: 45.615, Size: 18\n",
      "\tPopulation - \tAverage Fitness: -98.37328615185359\n",
      "\tmean = -98.37, \tmedian = -54.91, \tbest_median = 12.94\n",
      "\n",
      "Generation: 27\n",
      "\tBest - \tFitness: 72.653, Size: 31\n",
      "\tPopulation - \tAverage Fitness: -92.11508514954023\n",
      "\tmean = -92.12, \tmedian = -28.31, \tbest_median = 12.94\n",
      "\n",
      "Generation: 28\n",
      "\tBest - \tFitness: 53.836, Size: 16\n",
      "\tPopulation - \tAverage Fitness: -67.78998593657455\n",
      "\tmean = -67.79, \tmedian = 9.01, \tbest_median = 12.94\n",
      "\n",
      "Generation: 29\n",
      "\tBest - \tFitness: 81.355, Size: 24\n",
      "\tPopulation - \tAverage Fitness: -102.59165433270722\n",
      "\tmean = -102.59, \tmedian = -39.30, \tbest_median = 12.94\n",
      "\n",
      "Generation: 30\n",
      "\tBest - \tFitness: 49.080, Size: 15\n",
      "\tPopulation - \tAverage Fitness: -81.39432572585977\n",
      "\tmean = -81.39, \tmedian = -26.52, \tbest_median = 12.94\n",
      "\n",
      "Generation: 31\n",
      "\tBest - \tFitness: 74.442, Size: 27\n",
      "\tPopulation - \tAverage Fitness: -52.604581968095914\n",
      "\tmean = -52.60, \tmedian = 22.77, \tbest_median = 22.77\n",
      "\n",
      "Generation: 32\n",
      "\tBest - \tFitness: 81.871, Size: 18\n",
      "\tPopulation - \tAverage Fitness: -87.73358266915824\n",
      "\tmean = -87.73, \tmedian = 12.77, \tbest_median = 22.77\n",
      "\n",
      "Generation: 33\n",
      "\tBest - \tFitness: 97.665, Size: 16\n",
      "\tPopulation - \tAverage Fitness: -68.22146043760586\n",
      "\tmean = -68.22, \tmedian = 7.04, \tbest_median = 22.77\n",
      "\n",
      "Generation: 34\n",
      "\tBest - \tFitness: 88.443, Size: 16\n",
      "\tPopulation - \tAverage Fitness: -54.219046371665996\n",
      "\tmean = -54.22, \tmedian = 10.08, \tbest_median = 22.77\n",
      "\n",
      "Generation: 35\n",
      "\tBest - \tFitness: 122.105, Size: 18\n",
      "\tPopulation - \tAverage Fitness: -61.656763024752834\n",
      "\tmean = -61.66, \tmedian = 15.06, \tbest_median = 22.77\n",
      "\n",
      "Generation: 36\n",
      "\tBest - \tFitness: 79.618, Size: 28\n",
      "\tPopulation - \tAverage Fitness: -49.01764643010898\n",
      "\tmean = -49.02, \tmedian = 7.44, \tbest_median = 22.77\n",
      "\n",
      "Generation: 37\n",
      "\tBest - \tFitness: 74.391, Size: 30\n",
      "\tPopulation - \tAverage Fitness: -81.99956460654784\n",
      "\tmean = -82.00, \tmedian = 7.08, \tbest_median = 22.77\n",
      "\n",
      "Generation: 38\n",
      "\tBest - \tFitness: 95.493, Size: 30\n",
      "\tPopulation - \tAverage Fitness: -38.37301198444306\n",
      "\tmean = -38.37, \tmedian = 28.25, \tbest_median = 28.25\n",
      "\n",
      "Generation: 39\n",
      "\tBest - \tFitness: 78.214, Size: 22\n",
      "\tPopulation - \tAverage Fitness: -41.9354962391042\n",
      "\tmean = -41.94, \tmedian = 29.49, \tbest_median = 29.49\n",
      "\n",
      "Generation: 40\n",
      "\tBest - \tFitness: 68.180, Size: 23\n",
      "\tPopulation - \tAverage Fitness: -29.38117479328026\n",
      "\tmean = -29.38, \tmedian = 45.44, \tbest_median = 45.44\n",
      "\n",
      "Generation: 41\n",
      "\tBest - \tFitness: 78.450, Size: 31\n",
      "\tPopulation - \tAverage Fitness: -6.505325868074788\n",
      "\tmean = -6.51, \tmedian = 49.87, \tbest_median = 49.87\n",
      "\n",
      "Generation: 42\n",
      "\tBest - \tFitness: 69.850, Size: 20\n",
      "\tPopulation - \tAverage Fitness: 1.3207489886382187\n",
      "\tmean = 1.32, \tmedian = 60.54, \tbest_median = 60.54\n",
      "\n",
      "Generation: 43\n",
      "\tBest - \tFitness: 106.855, Size: 30\n",
      "\tPopulation - \tAverage Fitness: -14.571856338175161\n",
      "\tmean = -14.57, \tmedian = 50.65, \tbest_median = 60.54\n",
      "\n",
      "Generation: 44\n",
      "\tBest - \tFitness: 57.295, Size: 30\n",
      "\tPopulation - \tAverage Fitness: -14.228533867222847\n",
      "\tmean = -14.23, \tmedian = 34.90, \tbest_median = 60.54\n",
      "\n",
      "Generation: 45\n",
      "\tBest - \tFitness: 90.652, Size: 30\n",
      "\tPopulation - \tAverage Fitness: -50.69483906171653\n",
      "\tmean = -50.69, \tmedian = 19.44, \tbest_median = 60.54\n",
      "\n",
      "Generation: 46\n",
      "\tBest - \tFitness: 93.840, Size: 26\n",
      "\tPopulation - \tAverage Fitness: -0.2669317423492958\n",
      "\tmean = -0.27, \tmedian = 70.09, \tbest_median = 70.09\n",
      "\n",
      "Generation: 47\n",
      "\tBest - \tFitness: 53.261, Size: 28\n",
      "\tPopulation - \tAverage Fitness: -53.652702231989736\n",
      "\tmean = -53.65, \tmedian = 11.60, \tbest_median = 70.09\n",
      "\n",
      "Generation: 48\n",
      "\tBest - \tFitness: 107.817, Size: 31\n",
      "\tPopulation - \tAverage Fitness: -13.537128949876383\n",
      "\tmean = -13.54, \tmedian = 40.88, \tbest_median = 70.09\n",
      "\n",
      "Generation: 49\n",
      "\tBest - \tFitness: 85.573, Size: 28\n",
      "\tPopulation - \tAverage Fitness: -29.547137136606324\n",
      "\tmean = -29.55, \tmedian = 45.57, \tbest_median = 70.09\n",
      "\n",
      "Generation: 50\n",
      "\tBest - \tFitness: 64.990, Size: 31\n",
      "\tPopulation - \tAverage Fitness: -10.831104896234217\n",
      "\tmean = -10.83, \tmedian = 45.51, \tbest_median = 70.09\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'fitnesses-2023-06-14-17:57:29.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\shutil.py:825\u001b[0m, in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     os\u001b[39m.\u001b[39;49mrename(src, real_dst)\n\u001b[0;32m    826\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 87] De parameter is onjuist: 'fitnesses.csv' -> 'fitnesses-2023-06-14-17:57:29.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m      9\u001b[0m   evo \u001b[39m=\u001b[39m Evolution(\n\u001b[0;32m     10\u001b[0m     fitness_function_pt, internal_nodes, leaf_nodes,\n\u001b[0;32m     11\u001b[0m     \u001b[39m4\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     log_data\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 19\u001b[0m   evo\u001b[39m.\u001b[39;49mevolve()\n",
      "File \u001b[1;32mc:\\Users\\WK\\Documents\\GitHub\\EA-group-8\\genepro\\evo.py:298\u001b[0m, in \u001b[0;36mEvolution.evolve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_data:\n\u001b[0;32m    297\u001b[0m   dt, _ \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mutcnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS.\u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 298\u001b[0m   shutil\u001b[39m.\u001b[39;49mmove(\u001b[39m'\u001b[39;49m\u001b[39mfitnesses.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfitnesses-\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mdt\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    299\u001b[0m   shutil\u001b[39m.\u001b[39mmove(\u001b[39m'\u001b[39m\u001b[39mgen.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgen_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mdt\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\shutil.py:845\u001b[0m, in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    843\u001b[0m         rmtree(src)\n\u001b[0;32m    844\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 845\u001b[0m         copy_function(src, real_dst)\n\u001b[0;32m    846\u001b[0m         os\u001b[39m.\u001b[39munlink(src)\n\u001b[0;32m    847\u001b[0m \u001b[39mreturn\u001b[39;00m real_dst\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\shutil.py:444\u001b[0m, in \u001b[0;36mcopy2\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(dst):\n\u001b[0;32m    443\u001b[0m     dst \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dst, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(src))\n\u001b[1;32m--> 444\u001b[0m copyfile(src, dst, follow_symlinks\u001b[39m=\u001b[39;49mfollow_symlinks)\n\u001b[0;32m    445\u001b[0m copystat(src, dst, follow_symlinks\u001b[39m=\u001b[39mfollow_symlinks)\n\u001b[0;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m dst\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\shutil.py:266\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(src, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fsrc:\n\u001b[0;32m    265\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(dst, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fdst:\n\u001b[0;32m    267\u001b[0m             \u001b[39m# macOS\u001b[39;00m\n\u001b[0;32m    268\u001b[0m             \u001b[39mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[0;32m    269\u001b[0m                 \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'fitnesses-2023-06-14-17:57:29.csv'"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_features = env.observation_space.shape[0]\n",
    "leaf_nodes = [Feature(i) for i in range(num_features)] + [Constant() for _ in range(1)]\n",
    "internal_nodes = [Plus(), Minus(), Times(), Div(), Square(), Sqrt(), Log(), Sin(), Cos(), Max(), Min()] #Add your own operators here\n",
    "\n",
    "# Run a few times to collect Data\n",
    "for _ in range(5):\n",
    "  evo = Evolution(\n",
    "    fitness_function_pt, internal_nodes, leaf_nodes,\n",
    "    4,\n",
    "    pop_size=256,\n",
    "    max_gens=50,\n",
    "    max_tree_size=31,\n",
    "    n_jobs=8,\n",
    "    log_data=True,\n",
    "    verbose=True)\n",
    "  \n",
    "  evo.evolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('baseline.pickle', 'wb') as file:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(base_evo, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_score(tree):\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(10):\n",
    "      # get initial state\n",
    "      observation = env.reset(seed=i)\n",
    "      observation = observation[0]\n",
    "\n",
    "      for _ in range(500):    \n",
    "        # build up the input sample for GP\n",
    "        input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        # get output (squeezing because it is encapsulated in an array)\n",
    "        output = tree.get_output_pt(input_sample)\n",
    "        action = torch.argmax(output)\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "\n",
    "        output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        if (terminated or truncated):\n",
    "            break\n",
    "\n",
    "    fitness = np.mean(rewards)\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "best = evo.best_of_gens[-1]\n",
    "\n",
    "print(best.get_readable_repr())\n",
    "print(get_test_score(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an animation\n",
    "Here the best evolved individual is selected and one episode is rendered. Make sure to save your lunar landers over time to track progress and make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "# gist to save gif from https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553\n",
    "def save_frames_as_gif(frames, path='./', filename='evolved_lander.gif'):\n",
    "  plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "  patch = plt.imshow(frames[0])\n",
    "  plt.axis('off')\n",
    "  def animate(i):\n",
    "      patch.set_data(frames[i])\n",
    "  anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "  anim.save(path + filename, writer='imagemagick', fps=60)\n",
    "\n",
    "frames = []\n",
    "fitness_function_pt(best, generation = 0, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander.gif\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "The coefficients in the multi-tree aren't optimised. Here Q-learning (taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) is used to optimise the weights further. Incorporate coefficient optimisation in training your agent(s). Coefficient Optimisation can be expensive. Think about how often you want to optimise, when, which individuals etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "constants = best.get_subtrees_consts()\n",
    "\n",
    "if len(constants)>0:\n",
    "  optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "\n",
    "for _ in range(500):\n",
    "\n",
    "  if len(constants)>0 and len(evo.memory)>batch_size:\n",
    "    target_tree = copy.deepcopy(best)\n",
    "\n",
    "    transitions = evo.memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), dtype=torch.bool)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                               if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = best.get_output_pt(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "      next_state_values[non_final_mask] = target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "   \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "    optimizer.step()\n",
    "\n",
    "print(best.get_readable_repr())\n",
    "print(get_test_score(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "fitness_function_pt(best, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames, filename='evolved_lander_RL.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander_RL.gif\" width=\"750\">"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "224d5cd9c6b0ee09a4fcb288b4f99402d88b7b3d7b0cb6a08c577691994ebb89"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
