{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving a Lunar Lander with differentiable Genetic Programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:solution.ipynb
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:04:43.667278400Z",
     "start_time": "2023-06-15T13:04:43.656684700Z"
    }
   },
   "outputs": [],
   "source": [
    "install = False\n",
    "load_model = True\n",
    "merge_csv = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:04:43.689838900Z",
     "start_time": "2023-06-15T13:04:43.674829500Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture --no-display\n",
    "if install:\n",
    "    %pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Imports from the standard genepro-multi library are done here. Any adjustments (e.g. different operators) should be made in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:04:49.863593300Z",
     "start_time": "2023-06-15T13:04:43.698108500Z"
    }
   },
   "outputs": [],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python2 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from genepro.node_impl import *\n",
    "from genepro.42evo import Evolution\n",
    "# from genepro.evo import Evolution\n",
    "from genepro.node_impl import Constant\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from sympy import simplify\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
<<<<<<< HEAD:solution.ipynb
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Setup\n",
    "Here we first setup the Gymnasium environment. Please see https://gymnasium.farama.org/environments/box2d/lunar_lander/ for more information on the environment. \n",
    "\n",
    "Then a memory buffer is made. This is a buffer in which state transitions are stored. When the buffer reaches its maximum capacity old transitions are replaced by new ones.\n",
    "\n",
    "A frame buffer is initialised used to later store animation frames of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:04:50.006166Z",
     "start_time": "2023-06-15T13:04:49.869921500Z"
    }
   },
=======
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:solution.ipynb
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:04:50.069765500Z",
     "start_time": "2023-06-15T13:04:50.021102100Z"
    }
   },
=======
   "execution_count": 3,
   "metadata": {},
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "      self.memory += other.memory\n",
    "      return self \n",
    "\n",
    "    def __add__(self, other):\n",
    "      self.memory = self.memory + other.memory \n",
    "      return self"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:solution.ipynb
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:04:50.069765500Z",
     "start_time": "2023-06-15T13:04:50.034612500Z"
    }
   },
=======
   "execution_count": 4,
   "metadata": {},
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   "outputs": [],
   "source": [
    "frames = []"
   ]
  },
  {
   "attachments": {},
<<<<<<< HEAD:solution.ipynb
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the leaf nodes\n",
    "The gym environment returns states that represent where the lunar lander is at the current frame. These states are used to calculate the next move of the lander. For each state, we create a leaf node that can be used in the tree that represent the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment to find how many features it returns\n",
    "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
    "num_features = env.observation_space.shape[0]\n",
    "\n",
    "# Create a feature for each value the evironment returns\n",
    "leaf_nodes = [Feature(i) for i in range(num_features)]\n",
    "# Add random constants to the features\n",
    "leaf_nodes = leaf_nodes + [Constant()] \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_x = np.multiply(x,np.array([-1,1,-1,1,-1,-1,1,1,1,-1]))\n",
    "def transform_input(x):\n",
    "    if x[0,0]>= 0:\n",
    "        return x\n",
    "    else:\n",
    "         new_x = np.multiply(x,np.array([-1,1,-1,1,-1,-1,1,1]))\n",
    "         new_x[0,6] = x[0,7]\n",
    "         new_x[0,7] = x[0,6]\n",
    "    return new_x\n",
    "\n",
    "def transform_full_input(x):\n",
    "    if x[0,0]>= 0:\n",
    "        return x\n",
    "    else:\n",
    "         new_x = np.multiply(x,np.array([-1,1,-1,1,-1,-1,1,1,1,-1]))\n",
    "         new_x[0,6] = x[0,7]\n",
    "         new_x[0,7] = x[0,6]\n",
    "    return new_x\n",
    "\n",
    "def transform_action(x,action):\n",
    "    if x[0,0]>= 0:\n",
    "        return action\n",
    "    else:\n",
    "        return -action"
   ]
  },
  {
   "attachments": {},
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Function\n",
    "\n",
    "Here you get to be creative. The default setup evaluates 5 episodes of 300 frames. Think of what action to pick and what fitness function to use. The Multi-tree takes an input of $n \\times d$ where $n$ is a batch of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
<<<<<<< HEAD:solution.ipynb
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:04:50.069765500Z",
     "start_time": "2023-06-15T13:04:50.045055400Z"
    }
   },
=======
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fitness_function_pt(multitree, num_episodes=5, episode_duration=300, render=False, ignore_done=False):\n",
    "#   memory = ReplayMemory(10000)\n",
    "#   rewards = []\n",
    "\n",
    "#   for _ in range(num_episodes):\n",
    "#     # get initial state of the environment\n",
    "#     observation = env.reset()\n",
    "#     observation = observation[0]\n",
    "#     action = np.array([0,0])\n",
    "\n",
    "#     for _ in range(episode_duration):\n",
    "#       if render:\n",
    "#         frames.append(env.render())\n",
    "\n",
    "#       input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "      \n",
    "#       # Improvement symmetry\n",
    "#       # Mirror the input, and use that as the input\n",
    "#       input = observation.reshape((1,-1))       \n",
    "#       mirrored_input = transform_input(input)\n",
    "#       # outputs = multitree.get_child_outputs(mirrored_input) # Get the outputs for the mirrored input, does this work? --W\n",
    "#       outputs = multitree.get_output_pt(mirrored_input) # Get the outputs for the mirrored input, does this work? --W\n",
    "#       # outputs = multitree.get_child_outputs(input)\n",
    "#       outputs = multitree.get_output_pt(input)\n",
    "#       # outputs = multitree.get_child_outputs(outputs)\n",
    "\n",
    "#       main_engine = np.squeeze((outputs[0]))            \n",
    "#       side_engine = np.squeeze((outputs[1]))\n",
    "\n",
    "#       # action = torch.argmax(multitree.get_output_pt(input_sample))\n",
    "#       # action = np.array([main_engine, mirrored_side_engine])\n",
    "      \n",
    "#       # Transform the side engine back from the mirrored one, and use that for the action\n",
    "#       mirrored_side_engine = transform_action(input, side_engine)\n",
    "#       action = np.array([main_engine, mirrored_side_engine])\n",
    "#       # action = torch.argmax(np.array([main_engine, mirrored_side_engine]))\n",
    "#       # action to tensor \n",
    "#       action = torch.from_numpy(action).float()\n",
    "#       # observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "#       # observation, reward, done, _ = env.step(action)\n",
    "#       observation, reward, terminated, truncated, info =env.step(action.item())\n",
    "\n",
    "#       rewards.append(reward)\n",
    "#       output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "#       memory.push(input_sample, torch.tensor([[action.item()]]), output_sample, torch.tensor([reward]))\n",
    "#       if (terminated or truncated) and not ignore_done:\n",
    "#         break\n",
    "\n",
    "#   fitness = np.sum(rewards)\n",
    "  \n",
    "#   return fitness, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   "outputs": [],
   "source": [
    "def fitness_function_pt(multitree, num_episodes=5, episode_duration=300, render=False, ignore_done=False):\n",
    "  memory = ReplayMemory(10000)\n",
    "  rewards = []\n",
    "\n",
    "  for _ in range(num_episodes):\n",
    "    # get initial state of the environment\n",
    "    observation = env.reset()\n",
    "    observation = observation[0]\n",
    "    action = np.array([0,0])\n",
    "\n",
    "    for _ in range(episode_duration):\n",
    "      if render:\n",
    "        frames.append(env.render())\n",
    "\n",
    "      input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "      \n",
    "      # Improvement symmetry\n",
    "      # Mirror the input, and use that as the input\n",
    "      input = observation.reshape((1,-1))       \n",
    "      outputs = multitree.get_output_pt(torch.from_numpy(transform_input(input))) # Get the outputs for the mirrored input, does this work? --W\n",
    "      print(outputs.shape)\n",
    "\n",
    "      # !!! Transform the output from [1, 4] dimension tensor to [4] dimension array\n",
    "      [ do_nothing, left_thruster, main_thruster, right_thruster ] = np.squeeze(outputs.tolist())\n",
    "      \n",
    "      # !!! Transform the orientation of the right thruster based on location, and use that for the action???\n",
    "      mirrored_right_thruster = transform_action(input, right_thruster)\n",
    "\n",
    "      # Here we store the new action values, with the mirrored value replacing the right thruster.\n",
    "      actions = np.array([[do_nothing, left_thruster, main_thruster, mirrored_right_thruster]])\n",
    "        \n",
    "      # !!! Here you need some logic to pick the action\n",
    "      action = torch.argmax(torch.from_numpy(actions))\n",
    "\n",
    "      # !!! I tried the default way of picking actions, which also produces an error...\n",
    "      #action = torch.argmax(multitree.get_output_pt(input_sample))\n",
    "        \n",
    "      observation, reward, terminated, truncated, info =env.step(action.item())\n",
    "\n",
    "      rewards.append(reward)\n",
    "      output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "      memory.push(input_sample, torch.tensor([[action.item()]]), output_sample, torch.tensor([reward]))\n",
    "      if (terminated or truncated) and not ignore_done:\n",
    "        break\n",
    "\n",
    "  fitness = np.sum(rewards)\n",
    "  \n",
    "  return fitness, memory\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:solution.ipynb
    "# Evolution Setup & Evolve\n",
    "Here the leaf and internal nodes are defined. Think about the odds of sampling a constant in this default configurations. Also think about any operators that could be useful and add them here. \n",
    "\n",
    "Adjust the population size (multiple of 8 if you want to use the standard tournament selection), max generations and max tree size to taste. Be aware that each of these settings can increase the runtime."
=======
    "## Baseline"
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:solution.ipynb
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:04:50.088265100Z",
     "start_time": "2023-06-15T13:04:50.075568200Z"
    }
   },
   "outputs": [],
   "source": [
    "# if load_model:\n",
    "    # %%script false\n",
    "# else:\n",
    "if not load_model:\n",
    "  np.random.seed(42)\n",
    "\n",
    "  num_features = env.observation_space.shape[0]\n",
    "  leaf_nodes = [Feature(i) for i in range(num_features)] + [Constant() for _ in range(1)]\n",
    "  internal_nodes = [Plus(), Minus(), Times(), Div(), Square(), Sqrt(), Log(), Sin(), Cos(), Max(), Min()] #Add your own operators here\n",
    "\n",
    "  # Run a few times to collect Data\n",
    "  for i in range(5):\n",
    "    print(f\"------ Evolution {i} ------\")\n",
    "    \n",
    "    evo = Evolution(\n",
    "      fitness_function_pt, internal_nodes, leaf_nodes,\n",
    "      4,\n",
    "      pop_size=256,\n",
    "      max_gens=50,\n",
    "      max_tree_size=31,\n",
    "      n_jobs=8,\n",
    "      log_data=True,\n",
    "      verbose=True)\n",
    "    \n",
    "    evo.evolve()"
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 1\n",
      "\tBest - \tFitness: 0.000, Size: 12\n",
      "\tPopulation - \tAverage Fitness: 0.0\n",
      "\tmean = 0.00, \tmedian = 0.00, \tbest_median = 0.00\n",
      "\n",
      "torch.Size([1, 4])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m      9\u001b[0m   evo \u001b[39m=\u001b[39m Evolution(\n\u001b[0;32m     10\u001b[0m     fitness_function_pt, internal_nodes, leaf_nodes,\n\u001b[0;32m     11\u001b[0m     \u001b[39m4\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     log_data\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 23\u001b[0m   evo\u001b[39m.\u001b[39;49mevolve()\n",
      "File \u001b[1;32mc:\\Users\\WK\\Documents\\GitHub\\EA-group-8\\genepro\\evo.py:267\u001b[0m, in \u001b[0;36mEvolution.evolve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39m# Generational Loop\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_must_terminate():\n\u001b[1;32m--> 267\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_perform_generation()\n\u001b[0;32m    269\u001b[0m   \u001b[39m# Save Statistics of current gen\u001b[39;00m\n\u001b[0;32m    270\u001b[0m   stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatistics[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_gens \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]  \u001b[39m# Gens start at 1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WK\\Documents\\GitHub\\EA-group-8\\genepro\\evo.py:211\u001b[0m, in \u001b[0;36mEvolution._perform_generation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m offspring_population \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)(delayed(generate_offspring)\n\u001b[0;32m    205\u001b[0m   (t, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrossovers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutations, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoeff_opts, \n\u001b[0;32m    206\u001b[0m   parents, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minternal_nodes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleaf_nodes,\n\u001b[0;32m    207\u001b[0m   constraints\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mmax_tree_size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tree_size}) \n\u001b[0;32m    208\u001b[0m   \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m parents)\n\u001b[0;32m    210\u001b[0m \u001b[39m# evaluate each offspring and store its fitness \u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m fit_out \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(delayed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitness_function)(t, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_gens) \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m offspring_population)\n\u001b[0;32m    212\u001b[0m fit_out \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlist\u001b[39m, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfit_out)))\n\u001b[0;32m    214\u001b[0m memories \u001b[39m=\u001b[39m fit_out[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "Cell \u001b[1;32mIn[8], line 38\u001b[0m, in \u001b[0;36mfitness_function_pt\u001b[1;34m(multitree, num_episodes, episode_duration, render, ignore_done)\u001b[0m\n\u001b[0;32m     33\u001b[0m action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(torch\u001b[39m.\u001b[39mfrom_numpy(actions))\n\u001b[0;32m     35\u001b[0m \u001b[39m# !!! I tried the default way of picking actions, which also produces an error...\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39m#action = torch.argmax(multitree.get_output_pt(input_sample))\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39menv\u001b[39m.\u001b[39;49mstep(action\u001b[39m.\u001b[39;49mitem())\n\u001b[0;32m     40\u001b[0m rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     41\u001b[0m output_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(observation\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:214\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    215\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    216\u001b[0m     result, \u001b[39mtuple\u001b[39m\n\u001b[0;32m    217\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpects step result to be a tuple, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:494\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    491\u001b[0m dispersion \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnp_random\u001b[39m.\u001b[39muniform(\u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m, \u001b[39m+\u001b[39m\u001b[39m1.0\u001b[39m) \u001b[39m/\u001b[39m SCALE \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m)]\n\u001b[0;32m    493\u001b[0m m_power \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m--> 494\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuous \u001b[39mand\u001b[39;00m action[\u001b[39m0\u001b[39;49m] \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m    495\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuous \u001b[39mand\u001b[39;00m action \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    496\u001b[0m ):\n\u001b[0;32m    497\u001b[0m     \u001b[39m# Main engine\u001b[39;00m\n\u001b[0;32m    498\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuous:\n\u001b[0;32m    499\u001b[0m         m_power \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39mclip(action[\u001b[39m0\u001b[39m], \u001b[39m0.0\u001b[39m, \u001b[39m1.0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m1.0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m  \u001b[39m# 0.5..1.0\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_features = env.observation_space.shape[0]\n",
    "leaf_nodes = [Feature(i) for i in range(num_features)] + [Constant() for _ in range(1)]\n",
    "internal_nodes = [Plus(), Minus(), Times(), Div(), Square(), Sqrt(), Log(), Sin(), Cos(), Max(), Min()] #Add your own operators here\n",
    "\n",
    "# Run a few times to collect Data\n",
    "for _ in range(5):\n",
    "  evo = Evolution(\n",
    "    fitness_function_pt, internal_nodes, leaf_nodes,\n",
    "    4,\n",
    "    pop_size=16,\n",
    "    max_gens=2,\n",
    "    max_tree_size=31,\n",
    "    n_jobs=1,\n",
    "    # pop_size=256,\n",
    "    # max_gens=50,\n",
    "    # max_tree_size=31,\n",
    "    # n_jobs=8,\n",
    "    log_data=True,\n",
    "    verbose=True)\n",
    "  \n",
    "  evo.evolve()"
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:solution.ipynb
    "## Save the Evolution"
=======
    "## Save models"
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:solution.ipynb
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:04:50.145573600Z",
     "start_time": "2023-06-15T13:04:50.092460200Z"
    }
   },
   "outputs": [],
   "source": [
    "if not load_model:\n",
    "    with open('baseline.pickle', 'wb') as file:\n",
    "        pickle.dump(evo, file, pickle.HIGHEST_PROTOCOL)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('baseline.pickle', 'wb') as file:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(evo, file, pickle.HIGHEST_PROTOCOL)"
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Evolution"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:solution.ipynb
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:04:57.034376900Z",
     "start_time": "2023-06-15T13:04:50.108019200Z"
    }
   },
   "outputs": [],
   "source": [
    "evo = pickle.load(open('baseline.pickle', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing, visualisation of the results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring and representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:04:57.053275700Z",
     "start_time": "2023-06-15T13:04:57.043601500Z"
    }
   },
=======
   "execution_count": null,
   "metadata": {},
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   "outputs": [],
   "source": [
    "def get_test_score(tree):\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(10):\n",
    "      # get initial state\n",
    "      observation = env.reset(seed=i)\n",
    "      observation = observation[0]\n",
    "\n",
    "      for _ in range(500):    \n",
    "        # build up the input sample for GP\n",
    "        input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        # get output (squeezing because it is encapsulated in an array)\n",
    "        output = tree.get_output_pt(input_sample)\n",
    "        action = torch.argmax(output)\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "\n",
    "        output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        if (terminated or truncated):\n",
    "            break\n",
    "\n",
    "    fitness = np.sum(rewards)\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "best = evo.best_of_gens[-1]\n",
    "representation = best.get_readable_repr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:05:09.543249200Z",
     "start_time": "2023-06-15T13:04:57.059993500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default:\n",
      "sqrt(abs(x_1))\n",
      "simple:\n",
      "sqrt(Abs(x_1))\n",
      "\n",
      "\n",
      "default:\n",
      "((sqrt(abs(x_1))+(sqrt(abs(x_1))+max(log(abs(x_7)),((x_3*x_5)+x_6))))+(sin(((sqrt(abs(x_1))+x_0)-sin(x_6)))+max(x_3,x_3)))\n",
      "simple:\n",
      "x_3 + sin(x_0 - sin(x_6) + sqrt(Abs(x_1))) + 2*sqrt(Abs(x_1)) + Max(x_3*x_5 + x_6, log(Abs(x_7)))\n",
      "\n",
      "\n",
      "default:\n",
      "(max(x_7,(x_5)**2)+((log(abs(x_3))+max(sqrt(abs(log(abs(x_2)))),sqrt(abs(x_0))))+sin(cos(x_0))))\n",
      "simple:\n",
      "log(Abs(x_3)) + sin(cos(x_0)) + Max(x_5**2, x_7) + Max(sqrt(Abs(x_0)), sqrt(Abs(log(Abs(x_2)))))\n",
      "\n",
      "\n",
      "default:\n",
      "((x_5-(sqrt(abs(x_4))-cos(sqrt(abs(sqrt(abs(x_6)))))))/sin(min(cos((cos(x_6)/sin((x_4-x_4)))),x_5)))\n",
      "simple:\n",
      "(x_5 + cos(Abs(x_6)**(1/4)) - sqrt(Abs(x_4)))/sin(Min(x_5, cos(zoo*cos(x_6))))\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WK\\Anaconda3\\envs\\genepro23\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2913799299808232\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(representation)):\n",
    "    print(\"default:\")\n",
    "    print(representation[x])\n",
    "    print(\"simple:\")\n",
    "    simpl_repr = simplify(representation[x])\n",
    "    print(simpl_repr)\n",
    "    print(\"\\n\")\n",
    "print(\"Best score: \", get_test_score(best))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an animation\n",
    "Here the best evolved individual is selected and one episode is rendered. Make sure to save your lunar landers over time to track progress and make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD:solution.ipynb
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-15T13:05:18.020073300Z"
    }
   },
=======
   "metadata": {},
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "# gist to save gif from https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553\n",
    "def save_frames_as_gif(frames, path='./', filename='evolved_lander.gif'):\n",
    "  plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "  patch = plt.imshow(frames[0])\n",
    "  plt.axis('off')\n",
    "  def animate(i):\n",
    "      patch.set_data(frames[i])\n",
    "  anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "  anim.save(path + filename, writer='imagemagick', fps=60)\n",
    "\n",
    "frames = []\n",
    "fitness_function_pt(best, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play animation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander.gif\" width=\"750\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "The coefficients in the multi-tree aren't optimised. Here Q-learning (taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) is used to optimise the weights further. Incorporate coefficient optimisation in training your agent(s). Coefficient Optimisation can be expensive. Think about how often you want to optimise, when, which individuals etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-15T13:05:18.026618Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "constants = best.get_subtrees_consts()\n",
    "\n",
    "if len(constants)>0:\n",
    "  optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "\n",
    "for _ in range(500):\n",
    "\n",
    "  if len(constants)>0 and len(evo.memory)>batch_size:\n",
    "    target_tree = copy.deepcopy(best)\n",
    "\n",
    "    transitions = evo.memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), dtype=torch.bool)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                               if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = best.get_output_pt(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "      next_state_values[non_final_mask] = target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "   \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "    optimizer.step()\n",
    "\n",
    "print(best.get_readable_repr())\n",
    "print(get_test_score(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-15T13:05:18.033943200Z"
    }
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "fitness_function_pt(best, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames, filename='evolved_lander_RL.gif')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander_RL.gif\" width=\"750\">"
   ]
  }
 ],
 "metadata": {
<<<<<<< HEAD:solution.ipynb
  "interpreter": {
   "hash": "224d5cd9c6b0ee09a4fcb288b4f99402d88b7b3d7b0cb6a08c577691994ebb89"
  },
=======
>>>>>>> 42-symmetry-v2:42Symmetry_solution.ipynb
  "kernelspec": {
   "display_name": "genepro23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
