{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving a Lunar Lander with differentiable Genetic Programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from genepro.node_impl import *\n",
    "from genepro.evo import Evolution\n",
    "from genepro.node_impl import Constant\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "      self.memory += other.memory\n",
    "      return self \n",
    "\n",
    "    def __add__(self, other):\n",
    "      self.memory = self.memory + other.memory \n",
    "      return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the leaf nodes\n",
    "The gym environment returns states that represent where the lunar lander is at the current frame. These states are used to calculate the next move of the lander. For each state, we create a leaf node that can be used in the tree that represent the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment to find how many features it returns\n",
    "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
    "num_features = env.observation_space.shape[0]\n",
    "\n",
    "# Create a feature for each value the evironment returns\n",
    "leaf_nodes = [Feature(i) for i in range(num_features)]\n",
    "# Add random constants to the features\n",
    "leaf_nodes = leaf_nodes + [Constant()] \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_x = np.multiply(x,np.array([-1,1,-1,1,-1,-1,1,1,1,-1]))\n",
    "def transform_input(x):\n",
    "    if x[0,0]>= 0:\n",
    "        return x\n",
    "    else:\n",
    "         new_x = np.multiply(x,np.array([-1,1,-1,1,-1,-1,1,1]))\n",
    "         new_x[0,6] = x[0,7]\n",
    "         new_x[0,7] = x[0,6]\n",
    "    return new_x\n",
    "\n",
    "def transform_full_input(x):\n",
    "    if x[0,0]>= 0:\n",
    "        return x\n",
    "    else:\n",
    "         new_x = np.multiply(x,np.array([-1,1,-1,1,-1,-1,1,1,1,-1]))\n",
    "         new_x[0,6] = x[0,7]\n",
    "         new_x[0,7] = x[0,6]\n",
    "    return new_x\n",
    "\n",
    "def transform_action(x,action):\n",
    "    if x[0,0]>= 0:\n",
    "        return action\n",
    "    else:\n",
    "        return -action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Function\n",
    "\n",
    "Here you get to be creative. The default setup evaluates 5 episodes of 300 frames. Think of what action to pick and what fitness function to use. The Multi-tree takes an input of $n \\times d$ where $n$ is a batch of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_function_pt(multitree, num_episodes=5, episode_duration=300, render=False, ignore_done=False):\n",
    "  memory = ReplayMemory(10000)\n",
    "  rewards = []\n",
    "\n",
    "  for _ in range(num_episodes):\n",
    "    # get initial state of the environment\n",
    "    observation = env.reset()\n",
    "    observation = observation[0]\n",
    "    action = np.array([0,0])\n",
    "\n",
    "    for _ in range(episode_duration):\n",
    "      if render:\n",
    "        frames.append(env.render())\n",
    "\n",
    "      input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "      \n",
    "      # Improvement symmetry\n",
    "      # Mirror the input, and use that as the input\n",
    "      input = observation.reshape((1,-1))       \n",
    "      mirrored_input = transform_input(input)\n",
    "      # outputs = multitree.get_child_outputs(mirrored_input) # Get the outputs for the mirrored input, does this work? --W\n",
    "      outputs = multitree.get_output_pt(mirrored_input) # Get the outputs for the mirrored input, does this work? --W\n",
    "      # outputs = multitree.get_child_outputs(input)\n",
    "      outputs = multitree.get_output_pt(input)\n",
    "      # outputs = multitree.get_child_outputs(outputs)\n",
    "\n",
    "      main_engine = np.squeeze((outputs[0]))            \n",
    "      side_engine = np.squeeze((outputs[1]))\n",
    "\n",
    "      # action = torch.argmax(multitree.get_output_pt(input_sample))\n",
    "      # action = np.array([main_engine, mirrored_side_engine])\n",
    "      \n",
    "      # Transform the side engine back from the mirrored one, and use that for the action\n",
    "      mirrored_side_engine = transform_action(input, side_engine)\n",
    "      action = np.array([main_engine, mirrored_side_engine])\n",
    "      # action = torch.argmax(np.array([main_engine, mirrored_side_engine]))\n",
    "      # action to tensor \n",
    "      action = torch.from_numpy(action).float()\n",
    "      # observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "      # observation, reward, done, _ = env.step(action)\n",
    "      observation, reward, terminated, truncated, info =env.step(action)\n",
    "\n",
    "      rewards.append(reward)\n",
    "      output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "      memory.push(input_sample, torch.tensor([[action.item()]]), output_sample, torch.tensor([reward]))\n",
    "      if (terminated or truncated) and not ignore_done:\n",
    "        break\n",
    "\n",
    "  fitness = np.sum(rewards)\n",
    "  \n",
    "  return fitness, memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_features = env.observation_space.shape[0]\n",
    "leaf_nodes = [Feature(i) for i in range(num_features)] + [Constant() for _ in range(1)]\n",
    "internal_nodes = [Plus(), Minus(), Times(), Div(), Square(), Sqrt(), Log(), Sin(), Cos(), Max(), Min()] #Add your own operators here\n",
    "\n",
    "# Run a few times to collect Data\n",
    "for _ in range(5):\n",
    "  evo = Evolution(\n",
    "    fitness_function_pt, internal_nodes, leaf_nodes,\n",
    "    4,\n",
    "    pop_size=16,\n",
    "    max_gens=2,\n",
    "    max_tree_size=8,\n",
    "    n_jobs=6,\n",
    "    # pop_size=256,\n",
    "    # max_gens=50,\n",
    "    # max_tree_size=31,\n",
    "    # n_jobs=8,\n",
    "    log_data=True,\n",
    "    verbose=True)\n",
    "  \n",
    "  evo.evolve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('baseline.pickle', 'wb') as file:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(evo, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (168324889.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    action = # What goes here?\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_test_score(tree):\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(10):\n",
    "      # get initial state\n",
    "      observation = env.reset(seed=i)\n",
    "      observation = observation[0]\n",
    "\n",
    "      for _ in range(500):    \n",
    "        # build up the input sample for GP\n",
    "        input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        # get output (squeezing because it is encapsulated in an array)\n",
    "        output = tree.get_output_pt(input_sample)\n",
    "        action = torch.argmax(output)\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "\n",
    "        output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        if (terminated or truncated):\n",
    "            break\n",
    "\n",
    "    fitness = np.sum(rewards)\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "best = evo.best_of_gens[-1]\n",
    "\n",
    "print(best.get_readable_repr())\n",
    "print(get_test_score(best))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an animation\n",
    "Here the best evolved individual is selected and one episode is rendered. Make sure to save your lunar landers over time to track progress and make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "# gist to save gif from https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553\n",
    "def save_frames_as_gif(frames, path='./', filename='evolved_lander.gif'):\n",
    "  plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "  patch = plt.imshow(frames[0])\n",
    "  plt.axis('off')\n",
    "  def animate(i):\n",
    "      patch.set_data(frames[i])\n",
    "  anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "  anim.save(path + filename, writer='imagemagick', fps=60)\n",
    "\n",
    "frames = []\n",
    "fitness_function_pt(best, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play animation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander.gif\" width=\"750\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "The coefficients in the multi-tree aren't optimised. Here Q-learning (taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) is used to optimise the weights further. Incorporate coefficient optimisation in training your agent(s). Coefficient Optimisation can be expensive. Think about how often you want to optimise, when, which individuals etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "constants = best.get_subtrees_consts()\n",
    "\n",
    "if len(constants)>0:\n",
    "  optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "\n",
    "for _ in range(500):\n",
    "\n",
    "  if len(constants)>0 and len(evo.memory)>batch_size:\n",
    "    target_tree = copy.deepcopy(best)\n",
    "\n",
    "    transitions = evo.memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), dtype=torch.bool)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                               if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = best.get_output_pt(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "      next_state_values[non_final_mask] = target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "   \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "    optimizer.step()\n",
    "\n",
    "print(best.get_readable_repr())\n",
    "print(get_test_score(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "fitness_function_pt(best, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames, filename='evolved_lander_RL.gif')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander_RL.gif\" width=\"750\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genepro23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
